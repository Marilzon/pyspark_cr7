{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "307cefac-8e3b-4b35-8d0d-065a67edc867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark.sql.session import SparkSession, DataFrame\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import lit, max as spark_max, when, col, coalesce\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fe06011-38e9-43c3-8d90-4a438488e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = (\n",
    "    SparkSession.builder.appName(\"scd2\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb2dc348-ead0-4481-b562-c2be8c2a501c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 16:14:30 - __main__ - INFO - Spark session created successfully with Delta Lake configuration\n"
     ]
    }
   ],
   "source": [
    "# logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Spark session created successfully with Delta Lake configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d21c569-3e8f-4868-a13a-373f2f0e4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add metadata function\n",
    "def add_scd2_metadata(dataframe: DataFrame):\n",
    "    logger.info(\n",
    "        \"Adding SCD2 metadata columns: is_active, start_date, end_date, version\"\n",
    "    )\n",
    "    return (\n",
    "        dataframe.withColumn(\"is_active\", lit(1))\n",
    "        .withColumn(\"start_date\", coalesce(dataframe[\"created_date\"]))\n",
    "        .withColumn(\"end_date\", lit(\"\"))\n",
    "        .withColumn(\"version\", lit(1))\n",
    "    )\n",
    "\n",
    "\n",
    "def new_data_classifier(current_data: DataFrame, new_data: DataFrame):\n",
    "    current_data.createOrReplaceTempView(\"current_data\")\n",
    "    new_data.createOrReplaceTempView(\"new_data\")\n",
    "\n",
    "    changes_query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM new_data\n",
    "    EXCEPT\n",
    "    SELECT * EXCEPT(start_date, end_date, version, is_active)\n",
    "    FROM current_data\n",
    "    \"\"\"\n",
    "\n",
    "    return spark.sql(changes_query)\n",
    "\n",
    "\n",
    "def expire_old_register(target_table: str, data_to_update_df: DataFrame, ids: list):\n",
    "    data_to_update_df.createOrReplaceTempView(\"data_to_expire\")\n",
    "    join_conditions = \" AND \".join([f\"target.{id} = source.{id}\" for id in ids])\n",
    "\n",
    "    return spark.sql(\n",
    "        f\"\"\"\n",
    "        MERGE INTO {target_table} AS target\n",
    "        USING data_to_expire AS source\n",
    "        ON {join_conditions} AND target.is_active = 1\n",
    "        WHEN MATCHED THEN UPDATE SET\n",
    "            target.is_active = 0,\n",
    "            target.end_date = CURRENT_DATE()\n",
    "    \"\"\"\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d176c4d-c702-47de-a564-56f3806be6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 16:14:46 - __main__ - INFO - Reading animals data from CSV file: pyspark_cr7/scd2/data/animals.csv\n",
      "2025-10-12 16:14:53 - __main__ - INFO - dataframe loaded with 10 rows           \n"
     ]
    }
   ],
   "source": [
    "# animals data csv\n",
    "logger.info(\"Reading animals data from CSV file: pyspark_cr7/scd2/data/animals.csv\")\n",
    "current_data_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"data/animals.csv\")\n",
    ")\n",
    "\n",
    "logger.info(f\"dataframe loaded with {current_data_df.count()} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cae8864-ef04-43e0-b89b-60e4a2254a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 16:14:57 - __main__ - INFO - Adding SCD2 metadata columns: is_active, start_date, end_date, version\n",
      "2025-10-12 16:14:57 - __main__ - INFO - SCD2 metadata added successfully\n"
     ]
    }
   ],
   "source": [
    "# add metadata\n",
    "current_data_df = add_scd2_metadata(current_data_df)\n",
    "logger.info(\"SCD2 metadata added successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3c2174c-be0c-4e10-9ba7-2b5466b27dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/12 16:15:02 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "2025-10-12 16:15:14 - __main__ - INFO - Animals table successfully written to Delta format\n"
     ]
    }
   ],
   "source": [
    "# write animals table\n",
    "current_data_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"scd_type2_animals\")\n",
    "logger.info(\"Animals table successfully written to Delta format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93d11779-5682-463c-acb6-b9abbe3c15fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 16:15:14 - __main__ - INFO - Reading animals data from Delta table: scd_type2_animals\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------------+---------+----------+--------+-------+\n",
      "| id| animal|created_date|is_active|start_date|end_date|version|\n",
      "+---+-------+------------+---------+----------+--------+-------+\n",
      "|  1|    Cat|  2023-01-01|        1|2023-01-01|        |      1|\n",
      "|  2|    Cow|  2023-01-04|        1|2023-01-04|        |      1|\n",
      "|  3|    Dog|  2023-02-19|        1|2023-02-19|        |      1|\n",
      "|  4|  Horse|  2023-03-05|        1|2023-03-05|        |      1|\n",
      "|  5| Rabbit|  2023-03-15|        1|2023-03-15|        |      1|\n",
      "|  6|   Bird|  2023-04-10|        1|2023-04-10|        |      1|\n",
      "|  7|   Fish|  2023-05-22|        1|2023-05-22|        |      1|\n",
      "|  8|Hamster|  2023-06-08|        1|2023-06-08|        |      1|\n",
      "|  9| Turtle|  2023-07-14|        1|2023-07-14|        |      1|\n",
      "| 10| Lizard|  2023-08-30|        1|2023-08-30|        |      1|\n",
      "+---+-------+------------+---------+----------+--------+-------+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- animal: string (nullable = true)\n",
      " |-- created_date: string (nullable = true)\n",
      " |-- is_active: integer (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- version: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read animals table\n",
    "logger.info(\"Reading animals data from Delta table: scd_type2_animals\")\n",
    "current_data_df = spark.read.table(\"scd_type2_animals\")\n",
    "current_data_df.show()\n",
    "current_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "171be810-9aaa-4d75-b5cd-dc35ca81f76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 16:15:20 - __main__ - INFO - Reading updated animals data from CSV file: pyspark_cr7/scd2/data/updated_animals.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------------+\n",
      "| id|     animal|created_date|\n",
      "+---+-----------+------------+\n",
      "|  1|        Cat|  2025-01-01|\n",
      "|  2|        Cow|  2025-01-04|\n",
      "|  3|        Dog|  2025-02-19|\n",
      "|  4|      Horse|  2023-03-05|\n",
      "|  5|     Rabbit|  2023-03-15|\n",
      "|  6|       Bird|  2023-04-10|\n",
      "|  7|       Fish|  2023-05-22|\n",
      "|  8|    Hamster|  2025-06-08|\n",
      "|  9|Evil Turtle|  2023-07-14|\n",
      "| 10|  Charizard|  2023-08-30|\n",
      "| 11|        Mew|  2025-05-13|\n",
      "+---+-----------+------------+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- animal: string (nullable = true)\n",
      " |-- created_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# new animals data csv\n",
    "logger.info(\n",
    "    \"Reading updated animals data from CSV file: pyspark_cr7/scd2/data/updated_animals.csv\"\n",
    ")\n",
    "new_data_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"data/updated_animals.csv\")\n",
    ")\n",
    "\n",
    "new_data_df.show()\n",
    "new_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21007fb4-83a1-4cab-befd-a31fe18e500f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 16:15:57 - __main__ - INFO - Classify new data and expire old register\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------------+\n",
      "| id|     animal|created_date|\n",
      "+---+-----------+------------+\n",
      "|  1|        Cat|  2025-01-01|\n",
      "| 10|  Charizard|  2023-08-30|\n",
      "|  8|    Hamster|  2025-06-08|\n",
      "|  3|        Dog|  2025-02-19|\n",
      "| 11|        Mew|  2025-05-13|\n",
      "|  9|Evil Turtle|  2023-07-14|\n",
      "|  2|        Cow|  2025-01-04|\n",
      "+---+-----------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/12 16:16:03 WARN MapPartitionsRDD: RDD 120 was locally checkpointed, its lineage has been truncated and cannot be recomputed after unpersisting\n"
     ]
    }
   ],
   "source": [
    "# Classify new data and expire old register\n",
    "logger.info(\"Classify new data and expire old register\")\n",
    "\n",
    "data_to_update_df = new_data_classifier(\n",
    "    current_data=current_data_df, new_data=new_data_df\n",
    ")\n",
    "\n",
    "data_to_update_df.show()\n",
    "\n",
    "expire_old_register(\n",
    "    \"scd_type2_animals\", data_to_update_df, [\"id\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13b8fad3-91b5-495d-b435-c1fd42fb7068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------------+---------+----------+----------+-------+\n",
      "| id|     animal|created_date|is_active|start_date|  end_date|version|\n",
      "+---+-----------+------------+---------+----------+----------+-------+\n",
      "|  1|        Cat|  2023-01-01|        0|2023-01-01|2025-10-12|      1|\n",
      "|  2|        Cow|  2023-01-04|        0|2023-01-04|2025-10-12|      1|\n",
      "|  3|        Dog|  2023-02-19|        0|2023-02-19|2025-10-12|      1|\n",
      "|  4|      Horse|  2023-03-05|        1|2023-03-05|          |      1|\n",
      "|  5|     Rabbit|  2023-03-15|        1|2023-03-15|          |      1|\n",
      "|  6|       Bird|  2023-04-10|        1|2023-04-10|          |      1|\n",
      "|  7|       Fish|  2023-05-22|        1|2023-05-22|          |      1|\n",
      "|  8|    Hamster|  2023-06-08|        0|2023-06-08|2025-10-12|      1|\n",
      "|  9|     Turtle|  2023-07-14|        0|2023-07-14|2025-10-12|      1|\n",
      "| 10|     Lizard|  2023-08-30|        0|2023-08-30|2025-10-12|      1|\n",
      "|  1|        Cat|  2025-01-01|        1|2025-01-01|          |      2|\n",
      "| 10|  Charizard|  2023-08-30|        1|2023-08-30|          |      2|\n",
      "|  8|    Hamster|  2025-06-08|        1|2025-06-08|          |      2|\n",
      "|  3|        Dog|  2025-02-19|        1|2025-02-19|          |      2|\n",
      "| 11|        Mew|  2025-05-13|        1|2025-05-13|          |      1|\n",
      "|  9|Evil Turtle|  2023-07-14|        1|2023-07-14|          |      2|\n",
      "|  2|        Cow|  2025-01-04|        1|2025-01-04|          |      2|\n",
      "+---+-----------+------------+---------+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Append new regieters with metadata\n",
    "\n",
    "\n",
    "def append_new_scd2_data(updated_dataframe: DataFrame, table_name: str, ids: list):\n",
    "\n",
    "    latest_data_df = spark.read.table(table_name)\n",
    "    latest_data_df = current_data_df.groupBy(ids).agg(\n",
    "        spark_max(\"version\").alias(\"latest_version\")\n",
    "    )\n",
    "\n",
    "    join_condition = [id for id in ids]\n",
    "    \n",
    "    to_append_records_df = updated_dataframe.join(\n",
    "        latest_data_df, join_condition, \"left\"\n",
    "    )\n",
    "\n",
    "\n",
    "    to_append_records_df = (\n",
    "        to_append_records_df.withColumn(\n",
    "            \"version\",\n",
    "            when(col(\"latest_version\").isNull(), lit(1)).otherwise(\n",
    "                col(\"latest_version\") + 1\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\"start_date\", (col(\"created_date\")))\n",
    "        .withColumn(\"end_date\", lit(\"\"))\n",
    "        .withColumn(\"is_active\", lit(1))\n",
    "        .drop(\"latest_version\")\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        to_append_records_df.write.format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(table_name)\n",
    "    )\n",
    "\n",
    "append_new_scd2_data(\n",
    "    updated_dataframe=data_to_update_df, table_name=\"scd_type2_animals\", ids=[\"id\"]\n",
    ")\n",
    "\n",
    "\n",
    "spark.read.table(\"scd_type2_animals\").show()\n",
    "spark.stop()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
